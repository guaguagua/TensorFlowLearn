1  tf.argmax函数
此函数是对矩阵按行或列计算最大值的下标
tf.argmax(input, axis=None, name=None, dimension=None)
参数：
    input：输入Tensor
    axis：0表示按列，1表示按行
    name：名称
    dimension：和axis功能一样，默认axis取值优先。新加的字段
返回：Tensor  一般是行或列的最大值下标向量

import tensorflow as tf


a=tf.get_variable(name='a',
                  shape=[3,4],
                  dtype=tf.float32,
                  initializer=tf.random_uniform_initializer(minval=-1,maxval=1))
b=tf.argmax(input=a,axis=0)
c=tf.argmax(input=a,dimension=1)   #此处用dimesion或用axis是一样的
sess = tf.InteractiveSession()
sess.run(tf.initialize_all_variables())
print(sess.run(a))
#[[ 0.04261756 -0.34297419 -0.87816691 -0.15430689]
# [ 0.18663144  0.86972666 -0.06103253  0.38307118]
# [ 0.84588599 -0.45432305 -0.39736366  0.38526249]]
print(sess.run(b))
#[2 1 1 2]
print(sess.run(c))
#[0 1 0]

2 softmax python代码实现原理

输入向量 [1,2,3,4,1,2,3]
[1,2,3,4,1,2,3]}对应的Softmax函数的值为 
[ 0.024 , 0.064 , 0.175 , 0.475 , 0.024 , 0.064 , 0.175 ] 

输出向量中拥有最大权重的项对应着输入向量中的最大值“4”
这也显示了这个函数通常的意义：
对向量进行归一化，凸显其中最大的值并抑制远低于最大值的其他分量。

下面是使用Python进行函数计算的示例代码：

import math
z = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]
z_exp = [math.exp(i) for i in z]  
print(z_exp)  # Result: [2.72, 7.39, 20.09, 54.6, 2.72, 7.39, 20.09] 
sum_z_exp = sum(z_exp)  
print(sum_z_exp)  # Result: 114.98 
softmax = [round(i / sum_z_exp, 3) for i in z_exp]
print(softmax)  # Result: [0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]

如果不用指数运算，直接计算的话，就不能突出最大值，抑制远小于最大值的其他分量
所以进行指数运算是为了突出大值所占的比重
sum_z = sum(z)  #Result: 16
y = [i/sum_z for i in z] #Result: [0.0625, 0.125, 0.1875, 0.25, 0.0625, 0.125, 0.1875]
这种计算方式所得到的  4.0所占的比重是0.25 ，比softmax得到的0.475要小

3
在TensorFlow中文文档的教程中， 关于MNIST机器学习入门一节，开头down load mnist数据集的代码
import tensorflow.examples.tutorials.mnist.input_data
mnist = input_data.read_data_sets(“MNIST_data/”, one_hot=True)

直接复制运行会报错， NameError: name ‘input_data’ is not defined

应改成如下代码：

import tensorflow.examples.tutorials.mnist.input_data as input_data
mnist = input_data.read_data_sets(“MNIST_data/”, one_hot=True) 
